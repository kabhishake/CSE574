{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I (We) certify that the code and data in this assignment were generated independently, using only the tools and resources defined in the course and that I (we) did not receive any external help, coaching or contributions during the production of this work.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I (We) certify that the code and data in this assignment were generated independently, using only the tools and resources defined in the course and that I (we) did not receive any external help, coaching or contributions during the production of this work.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"penguins.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species               0\n",
       "island                0\n",
       "bill_length_mm        2\n",
       "bill_depth_mm         2\n",
       "flipper_length_mm     2\n",
       "body_mass_g           2\n",
       "sex                  11\n",
       "year                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['species'] = df['species'].astype('category')\n",
    "df['island'] = df['island'].astype('category')\n",
    "df['sex'] = df['sex'].astype('category')\n",
    "df['year'] = df['year'].astype('category') \n",
    "\n",
    "df_normalized = df.copy()\n",
    "\n",
    "for col in df_normalized.columns:\n",
    "    if df_normalized[col].dtypes == float:\n",
    "        df_normalized[col] = (df_normalized[col] - df_normalized[col].min()) / (df_normalized[col].max() - df_normalized[col].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>species_Chinstrap</th>\n",
       "      <th>species_Gentoo</th>\n",
       "      <th>island_Dream</th>\n",
       "      <th>island_Torgersen</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>year_2008</th>\n",
       "      <th>year_2009</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254545</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.269091</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.298182</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.152778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.167273</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.261818</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n",
       "0        0.254545       0.666667           0.152542     0.291667   \n",
       "1        0.269091       0.511905           0.237288     0.305556   \n",
       "2        0.298182       0.583333           0.389831     0.152778   \n",
       "4        0.167273       0.738095           0.355932     0.208333   \n",
       "5        0.261818       0.892857           0.305085     0.263889   \n",
       "\n",
       "   species_Chinstrap  species_Gentoo  island_Dream  island_Torgersen  \\\n",
       "0                  0               0             0                 1   \n",
       "1                  0               0             0                 1   \n",
       "2                  0               0             0                 1   \n",
       "4                  0               0             0                 1   \n",
       "5                  0               0             0                 1   \n",
       "\n",
       "   sex_male  year_2008  year_2009  \n",
       "0         1          0          0  \n",
       "1         0          0          0  \n",
       "2         0          0          0  \n",
       "4         0          0          0  \n",
       "5         1          0          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normalized = pd.get_dummies(df_normalized, drop_first=True)\n",
    "\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df_normalized[['sex_male']]\n",
    "X = df_normalized.drop(['sex_male'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, 10) (67, 10) (266,) (67,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test\n",
    "np.random.seed(8)\n",
    "\n",
    "train_size = int(0.8 * Y.shape[0])\n",
    "indis = list(range(Y.shape[0]))\n",
    "np.random.shuffle(indis)\n",
    "# print(indis)\n",
    "\n",
    "train_indis = indis[:train_size]\n",
    "test_indis = indis[train_size:]\n",
    "\n",
    "y_train, y_test = Y.iloc[train_indis], Y.iloc[test_indis]\n",
    "X_train, X_test = X.iloc[train_indis], X.iloc[test_indis]\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1)\n",
    "y_test = y_test.to_numpy().reshape(-1)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Perceptron From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron():\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        self.z = z\n",
    "        return 1 / (1 + np.exp(-self.z))\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X = np.insert(X_train, 0, 1, axis=1) # adding a column for bias term\n",
    "        self.y = y_train\n",
    "        self.w = np.ones(self.X.shape[1])\n",
    "        # print(self.w.shape, self.X.shape)\n",
    "        for i in range(self.epochs):\n",
    "            self.forwardpropagate()\n",
    "            self.backpropagate()\n",
    "        return self.w\n",
    "        \n",
    "    def forwardpropagate(self):\n",
    "        self.pred = self.sigmoid(np.dot(self.X, self.w))\n",
    "        self.error = -(self.y - self.pred)\n",
    "        # print(self.error)\n",
    "        \n",
    "    def backpropagate(self):\n",
    "        # print(self.error * self.pred * (1 - self.pred))\n",
    "        dw = np.dot(self.error * self.pred * (1 - self.pred), self.X)\n",
    "        self.w = self.w - self.learning_rate * dw\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        self.X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "        return np.where(self.sigmoid(np.dot(self.X_test, self.w)) >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9398496240601504\n",
      "0.8805970149253731\n"
     ]
    }
   ],
   "source": [
    "model1 = perceptron(learning_rate = 1, epochs = 1000)\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred_train = model1.predict(X_train)\n",
    "y_pred_test = model1.predict(X_test)\n",
    "\n",
    "acc_train = sum(y_pred_train == y_train)/len(y_train)\n",
    "print(acc_train)\n",
    "\n",
    "acc_test = sum(y_pred_test == y_test)/len(y_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9548872180451128\n",
      "0.8507462686567164\n"
     ]
    }
   ],
   "source": [
    "model2 = perceptron(learning_rate = 0.1, epochs = 10000)\n",
    "model2.fit(X_train, y_train)\n",
    "y_pred_train = model2.predict(X_train)\n",
    "y_pred_test = model2.predict(X_test)\n",
    "\n",
    "acc_train = sum(y_pred_train == y_train)/len(y_train)\n",
    "print(acc_train)\n",
    "\n",
    "acc_test = sum(y_pred_test == y_test)/len(y_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48872180451127817\n",
      "0.5223880597014925\n"
     ]
    }
   ],
   "source": [
    "model3 = perceptron(learning_rate = 10, epochs = 10000)\n",
    "model3.fit(X_train, y_train)\n",
    "y_pred_train = model3.predict(X_train)\n",
    "y_pred_test = model3.predict(X_test)\n",
    "\n",
    "acc_train = sum(y_pred_train == y_train)/len(y_train)\n",
    "print(acc_train)\n",
    "\n",
    "acc_test = sum(y_pred_test == y_test)/len(y_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9624060150375939\n",
      "0.8656716417910447\n"
     ]
    }
   ],
   "source": [
    "model4 = perceptron(learning_rate = 1, epochs = 10000)\n",
    "model4.fit(X_train, y_train)\n",
    "y_pred_train = model4.predict(X_train)\n",
    "y_pred_test = model4.predict(X_test)\n",
    "\n",
    "acc_train = sum(y_pred_train == y_train)/len(y_train)\n",
    "print(acc_train)\n",
    "\n",
    "acc_test = sum(y_pred_test == y_test)/len(y_test)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance and comparison with Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I used a perceptron with sigmoid activation function for the model\n",
    "- For error term I took the squared error function $\\frac{1}{2}(y-\\hat{y})^2$ even though this is a classification problem since it's easier to calculate the derivative of this error term. Also, logistic regression can be taken as a linear model since the output depends on the linear combination of features.\n",
    "- For backpropagation the calculated formula was: $\\frac{\\partial{E}}{\\partial{w}} = -(y-\\hat{y}) a (1-a) x$ where $a = \\frac{1}{1+e^{-z}}$, and x corresponds to weight w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model performance was fairly good on the Penguin dataset even though it was a single perceptron\n",
    "- We can note that if the learning rate is very small, the model doesn't converge to best parameters fast enough. On the other hand a large learning rate makes the model diverge\n",
    "- Similarly, when I used insufficient number of epochs the model didn't converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my best performing model the training accuracy was 96% and test accuracy was 86.5% (assuming that one model where test accuracy is 88% but training accuracy is just 93% is just due to a bad split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only slightly worse than the logistic regression model implemented using gradient descent where the accuracy was 88%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
